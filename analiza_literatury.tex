\chapter{Analiza literatury dotyczącej handlu algorytmicznego}

Przekrojową analizę zagadnienia handlu algorytmicznego przedstawił Michael Halls-Moore w książce \textit{Successful Algorithmic Trading} \parencite{halls2015}. Autor podkreśla znaczenie testów wstecznych (\emph{backtesting}) jako niezbędnego etapu weryfikacji strategii handlowych, omawia także zagadnienia analizy ryzyka i zarządzania portfelem inwestycyjnym. W dalszej części przedstawia implementację silnika \emph{Event-Driven Trading}, umożliwiającego reagowanie systemu na napływające dane rynkowe w czasie rzeczywistym. Halls-Moore opisuje również trzy przykładowe strategie inwestycyjne dla autonomicznych algorytmów handlowych: strategię przecięcia średnich kroczących, strategię predykcji zmian cen oraz strategię handlu parami opartą na odwróceniu do średniej, charakterystyczną dla systemów wysokiej częstotliwości (HFT).

W literaturze dostępnych jest wiele opracowań poświęconych zastosowaniu sztucznej inteligencji w handlu algorytmicznym, jednak rzadko autorzy analizują problem od strony inżynierskiej, opisując architekturę i implementację kompletnych systemów. Takie podejście zaprezentował Tri Nam Do w swojej pracy inżynierskiej \parencite{do2021}, w której przedstawił system autonomicznego algorytmu handlowego składający się z pięciu współpracujących modułów. Pierwszy z nich – \emph{Market API} – odpowiada za pozyskiwanie bieżących danych rynkowych oraz wykonywanie zleceń kupna i sprzedaży. Drugi moduł stanowi baza danych, gromadząca dane historyczne, rejestry transakcji oraz wyniki treningów modeli. Trzeci, moduł decyzyjny, wykorzystuje modele uczenia maszynowego i wskaźniki analizy technicznej do podejmowania decyzji inwestycyjnych. Czwarty moduł, logiczny, odpowiada za przygotowanie danych do treningu oraz integrację wyników modeli z historią transakcji, a następnie przekazuje decyzje do \emph{Market API}. Całość koordynowana jest przez moduł orkiestratora, który steruje procesami trenowania i zbierania danych. System został wdrożony w środowisku kontenerowym Docker i hostowany w chmurze AWS.

W tej samej pracy przetestowano strategię opartą na analizie technicznej, wykorzystującą przecięcie 10- i 15-dniowej średniej kroczącej jako sygnały kupna i sprzedaży. W okresie trzyletniego testu strategia wygenerowała zysk na poziomie 250\%, jednak po korekcie o wpływ silnych wzrostów kursu akcji AAPL w pierwszych dwóch latach, realna skuteczność w bardziej zmiennych warunkach rynkowych spadła do około 10\%.

Zastosowanie modeli zespołowych (\emph{ensemble learning}) w handlu walutowym omówił zespół badaczy pod kierunkiem Anastasiosa Petropoulosa \parencite{petro2017}. Autorzy połączyli prognozy z modeli SVM, Random Forest, BART, sieci neuronowych oraz klasyfikatora Naive Bayes dla dziesięciu par walutowych z USD jako walutą bazową. W okresie piętnastoletnim strategia uzyskała średni roczny zwrot na poziomie 18\%, uwzględniając koszty transakcyjne i dźwignię finansową. Metoda wykorzystała okno przesuwne do dynamicznej aktualizacji modeli oraz mechanizmy \emph{stop-loss} w celu ograniczenia ryzyka.

W ostatnich latach pojawiło się wiele publikacji analizujących zastosowanie głębokich sieci neuronowych w prognozowaniu kursów walut. W pracy przeglądowej Zexin Hu i współautorzy \textit{A Survey of Forex and Stock Price Prediction Using Deep Learning} \parencite{zexin2021} dokonali systematycznej analizy modeli stosowanych w badaniach z lat 2015–2021. Autorzy wskazują, że modele LSTM (Long Short-Term Memory) osiągają najlepsze wyniki w prognozowaniu trendów rynkowych dzięki zdolności do modelowania zależności czasowych. W zakresie zarządzania portfelem najwyższą efektywność uzyskały algorytmy oparte na uczeniu ze wzmocnieniem, w szczególności A3C (Asynchronous Advantage Actor-Critic), które umożliwiają dynamiczne dostosowanie poziomu ryzyka w czasie rzeczywistym.

Na uwagę zasługuje również oryginalne podejście Ömera Berata Sezera opisane w artykule \textit{Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach} \parencite{sezer2018}. Autor przekształcił szeregi czasowe cen w obrazy dwuwymiarowe, które następnie klasyfikował przy użyciu konwolucyjnej sieci neuronowej (CNN). Model został przetestowany na danych funduszy ETF z lat 2007–2017, osiągając ponad 70\% skuteczności oraz wyniki przewyższające strategie \emph{Buy \& Hold} i modele LSTM.

Zastosowanie uczenia ze wzmocnieniem w handlu algorytmicznym zostało szczegółowo opisane w pracy Thibauta Theate’a i współautorów \textit{An Application of Deep Reinforcement Learning to Algorithmic Trading} \parencite{theate2021}. Autorzy wykorzystali algorytm TDQN (Temporal-Difference Q-Network) do podejmowania decyzji handlowych w oparciu o dane OHLCV, wskaźniki techniczne oraz pozycje portfela. Wyniki eksperymentów wykazały, że TDQN uzyskał wyższe wskaźniki Sharpe’a w porównaniu z klasycznymi strategiami \emph{Buy \& Hold} oraz przecięć średnich kroczących, choć skuteczność algorytmu zależała od zmienności rynku.

Koncepcję strategii podążania za trendem (\emph{trend-following}) w kontekście handlu algorytmicznego omówili Simon Fong i współautorzy \parencite{fong2012}. Autorzy przeanalizowali różne odmiany tej strategii, wskazując, że najlepiej sprawdza się algorytm \emph{Recall}, wykorzystujący wzorce historycznych trendów do podejmowania decyzji w czasie rzeczywistym. W badaniach opartych na danych z okresu 2,5 roku uzyskano trzykrotnie wyższy zwrot z inwestycji (ROI) w porównaniu z klasycznymi strategiami trendowymi.

Coraz większe znaczenie w literaturze zyskuje analiza nastrojów (\emph{sentiment analysis}) w kontekście decyzji inwestycyjnych. W pracy Ashwini et al. \textit{Sentiment Analysis in Financial Markets} \parencite{ashwini2024} porównano skuteczność różnych modeli klasyfikacji nastrojów – najwyższe wyniki uzyskały nowoczesne modele językowe BERT i GPT, osiągając dokładność powyżej 90\%. Zbliżone rezultaty uzyskały sieci LSTM i CNN (ok. 86\%), podczas gdy klasyczne modele SVM i Naive Bayes osiągnęły odpowiednio 72\% i 90\% skuteczności. Wcześniejsze badania Johana Bollena \parencite{bollen2011} wykazały korelację między nastrojami publikowanymi na platformie Twitter a zmianami indeksu DJIA, wskazując, że włączenie wskaźników sentymentu do modeli predykcyjnych poprawia ich skuteczność nawet o 6–8 punktów procentowych.

Najnowsza literatura wskazuje na trzy wyraźne kierunki rozwoju: (i) przejście od klasycznych modeli ML do \emph{głębokich, multimodalnych} architektur (teksty, dane rynkowe LOB, wiadomości), (ii) szybki postęp w \emph{uczeniu ze wzmocnieniem} (RL/DRL) w zadaniach decyzyjnych o znaczeniu mikrostrukturalnym (wykonanie zleceń, market making, hedging), oraz (iii) rosnące znaczenie \emph{ładu i bezpieczeństwa modeli} (robustność, wyjaśnialność, ryzyko niezamierzonych zachowań agentów).

Po pierwsze, w predykcji i klasyfikacji sygnałów rośnie rola modeli głębokich łączących dane ilościowe z tekstem (\emph{multimodal learning}). Badania przeglądowe i porównawcze pokazują przewagę architektur opartych na \emph{transformerach} (BERT/FinBERT, modele klasy GPT) w analizie nastrojów finansowych oraz integracji informacji tekstowych z cechami technicznymi \parencite{zexin2021, nasiopoulos2025, kang2025}. Z kolei po stronie mikrostruktury, istotnym kamieniem milowym pozostaje \emph{DeepLOB} — konwolucyjne sieci dla książki zleceń, rozwijane następnie w ujęciu bayesowskim i portfelowym \parencite{zhang2019}. Modele te przenikają obecnie do zastosowań portfelowych oraz hybryd tekst–cena.

Po drugie, \emph{uczenie ze wzmocnieniem} stało się standardowym instrumentem dla problemów decyzyjnych w finansach: od optymalnego wykonania, przez market making, po dynamiczne zarządzanie portfelem i hedging \parencite{hambly2023}. Nowsze prace proponują \emph{wielomodalne} reprezentacje stanów (łączenie cen, wolumenu, cech mikrostrukturalnych, a nawet reprezentacji tekstu) oraz kryteria ryzyko–zysk, co poprawia wyniki względem strategii regułowych i klasycznych metod SL \parencite{avramelou2024}. Jednocześnie utrzymuje się znaczenie klasycznych benchmarków „statistical arbitrage” na danych akcyjnych (DNN/GBT/RF oraz ich zespoły), które stanowią punkt odniesienia dla nowych metod \parencite{krauss2017}.

Po trzecie, dynamicznie rośnie tematyka \emph{ryzyk systemowych i regulacyjnych} w kontekście agentów RL. Prace teoretyczne i eksperymenty symulacyjne wskazują możliwość \emph{emergentnej koluzji algorytmicznej} w środowiskach wieloagentowych, nawet bez bezpośredniej komunikacji, co ma konsekwencje dla efektywności cenowej i dobrostanu inwestorów detalicznych \parencite{dou2025}. Wątek ten wzmacnia debatę o wymogach nadzorczych (monitoring, testy warunków skrajnych, audytowalność) oraz o potrzebie procedur MLOps/Model Risk Management dla systemów handlujących autonomicznie \parencite{hambly2023, vancsura2025}.

Podsumowując, trajektoria rozwoju idzie w stronę \emph{hybryd}: łączenia sygnałów fundamentalno–tekstowych (transformery, FinBERT/GPT) z sygnałami mikrostrukturalnymi (DeepLOB) i decyzyjnością DRL, przy jednoczesnym adresowaniu problemów \emph{kosztów transakcyjnych, płynności, zarządzania ryzykiem} oraz \emph{robustności} na zmiany reżimów rynkowych \parencite{zexin2021, hambly2023, zhang2019, avramelou2024, vancsura2025}.

W ujęciu historycznym literatura przesuwała akcent od prostych reguł (przecięcia średnich kroczących, \emph{pairs trading}, arbitraż statystyczny) ku \emph{modelom zespołowym} oraz \emph{głębokim} (DNN, LSTM, CNN) i wreszcie ku \emph{architekturom transformerowym} oraz \emph{DRL} \parencite{krauss2017, zexin2021, hambly2023}. Najnowsze prace łączą reprezentacje tekstu (BERT/FinBERT/GPT) z sygnałami mikrostruktury (LOB) i decyzyjnością RL, co pozwala modelom adaptować się do zmiennych warunków, lecz jednocześnie zwiększa podatność na błędy specyfikacji, \emph{overfitting} oraz zachowania emergentne agentów \parencite{zhang2019, dou2025}. 

\textbf{Luka badawcza} rysuje się na styku: (i) \emph{spójnego MLOps/Model Risk Management} dla algorytmów tradingowych (walidacja ex–ante i ex–post, testy na reżimy, szacowanie kosztów i poślizgów), (ii) \emph{wyjaśnialności i zgodności regulacyjnej} dla modeli multimodalnych i DRL (metryki stabilności, śledzenie wpływu danych tekstowych na decyzje), oraz (iii) \emph{odporności wieloagentowych} środowisk rynkowych na koluzję algorytmiczną i inne zjawiska systemowe \parencite{hambly2023, vancsura2025, dou2025}. Zaproponowany w tej pracy prototyp \emph{autonomicznego systemu handlu} ukierunkowany na integrację sygnałów fundamentalnych/tekstowych z mikrostrukturą oraz na bezpieczne uczenie polityk (DRL) adresuje te braki poprzez jednoznaczny łańcuch walidacji, kontrolę ryzyka i monitorowanie zachowań agentów.

